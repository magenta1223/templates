{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_API_KEY=$cccc7c8a243b0a2734f74496bc3270f5ebf11d7c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# login\n",
    "import wandb\n",
    "\n",
    "%env WANDB_API_KEY=$cccc7c8a243b0a2734f74496bc3270f5ebf11d7c\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from modules import *\n",
    "from utils import *\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import yaml\n",
    "\n",
    "\n",
    "# for mixed precision\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")\n",
    "x_train = torch.tensor(mnist.data[:60000].values, dtype=torch.float) / 255\n",
    "y_train = torch.tensor([int(x) for x in mnist.target[:60000]])\n",
    "x_test = torch.tensor(mnist.data[60000:].values, dtype=torch.float) / 255\n",
    "y_test = torch.tensor([int(x) for x in mnist.target[60000:]])\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048)\n",
    "\n",
    "valid_dataset = TensorDataset(x_test, y_test)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    # ------------------- WRITE CODE ------------------ # \n",
    "    def __init__(self, hparams):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # -----------------------------MODEL--------------------------- #    \n",
    "\n",
    "        self.layers = nn.Sequential(nn.Linear(784, 10))\n",
    "        \n",
    "        # -----------------------------Optimizer---------------------------- #\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accumulative_step = 1 # hparams.accumulative_step\n",
    "\n",
    "        # -----------------------------Scheduler---------------------------- #\n",
    "        self.scheduler = hparams.SchedulerClass(self.optimizer, **hparams.scheduler_params) # scheduler\n",
    "\n",
    "\n",
    "        # -----------------------------Metric---------------------------- #\n",
    "\n",
    "        self.loss_meters = {'loss' : AverageMeter()}\n",
    "\n",
    "\n",
    "        # mixed precision\n",
    "\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------- Forward Pass ------- # \n",
    "        \n",
    "\n",
    "        return self.layers(x)        \n",
    "\n",
    "    def optimize(self, batch, iter):\n",
    "        # -------------- Train ----------------- # \n",
    "\n",
    "        inputs, targets = batch\n",
    "\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "\n",
    "        with autocast():\n",
    "            out = self(inputs) \n",
    "            loss = self.loss_fn(out, targets ) / self.accumulative_step\n",
    "\n",
    "\n",
    "        # ----------- DO NOT EDIT ------------ # \n",
    "        self.scaler.scale(loss).backward() # mixed precision\n",
    "\n",
    "\n",
    "        # gradient accumulation \n",
    "        if not (iter + 1) % self.accumulative_step: \n",
    "            #self.optimizer.step() \n",
    "            self.scaler.step(self.optimizer) # mixed precision       \n",
    "            self.scaler.update()   \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        self.loss_log(loss, batch[0].size(0) )\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self, batch):\n",
    "        # -------------- validate ----------------- # \n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "\n",
    "        out = self(inputs) \n",
    "        loss = self.loss_fn(out, targets)   \n",
    "        self.loss_log(loss, batch[0].size(0) )\n",
    "\n",
    "    \n",
    "    # --------------- DO NOT EDIT ----------------- #\n",
    "    def loss_log(self, loss, batch_size, key = 'loss'):\n",
    "        self.loss_meters[key].update(loss.item() * self.accumulative_step, batch_size )\n",
    "\n",
    "    def loss_dict(self, set_name):\n",
    "        return { f'{set_name.upper()}_{k}' : v.avg for k, v in self.loss_meters.items() }\n",
    "    \n",
    "    def meter_initialize(self):\n",
    "        for k, v in self.loss_meters.items():\n",
    "            v.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0]TRAIN loss : 1.76004 VALID loss : 1.27225 Time : 0.76362 s\n",
      "[Epoch 1]TRAIN loss : 1.06685 VALID loss : 0.86234 Time : 0.77167 s\n",
      "[Epoch 2]TRAIN loss : 0.79311 VALID loss : 0.68711 Time : 0.71899 s\n",
      "[Epoch 3]TRAIN loss : 0.66137 VALID loss : 0.59164 Time : 0.77499 s\n",
      "[Epoch 4]TRAIN loss : 0.58331 VALID loss : 0.53090 Time : 0.85610 s\n",
      "[Epoch 5]TRAIN loss : 0.53104 VALID loss : 0.48858 Time : 0.71997 s\n",
      "[Epoch 6]TRAIN loss : 0.49333 VALID loss : 0.45728 Time : 0.76300 s\n",
      "[Epoch 7]TRAIN loss : 0.46469 VALID loss : 0.43315 Time : 0.76297 s\n",
      "[Epoch 8]TRAIN loss : 0.44212 VALID loss : 0.41394 Time : 0.84607 s\n",
      "[Epoch 9]TRAIN loss : 0.42382 VALID loss : 0.39826 Time : 0.88597 s\n",
      "[Epoch 10]TRAIN loss : 0.40865 VALID loss : 0.38522 Time : 0.95927 s\n",
      "[Epoch 11]TRAIN loss : 0.39583 VALID loss : 0.37420 Time : 0.73465 s\n",
      "[Epoch 12]TRAIN loss : 0.38485 VALID loss : 0.36475 Time : 0.74810 s\n",
      "[Epoch 13]TRAIN loss : 0.37531 VALID loss : 0.35656 Time : 0.73594 s\n",
      "[Epoch 14]TRAIN loss : 0.36695 VALID loss : 0.34940 Time : 0.83994 s\n",
      "[Epoch 15]TRAIN loss : 0.35954 VALID loss : 0.34308 Time : 0.81605 s\n",
      "[Epoch 16]TRAIN loss : 0.35293 VALID loss : 0.33746 Time : 0.84491 s\n",
      "[Epoch 17]TRAIN loss : 0.34698 VALID loss : 0.33243 Time : 0.84275 s\n",
      "[Epoch 18]TRAIN loss : 0.34160 VALID loss : 0.32791 Time : 0.81195 s\n",
      "[Epoch 19]TRAIN loss : 0.33671 VALID loss : 0.32382 Time : 0.80604 s\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{BASE_DIR}/config.yaml\") as f:\n",
    "    hparams = edict(yaml.load(f))\n",
    "\n",
    "hparams.save_path = BASE_DIR, # model save path\n",
    "\n",
    "hparams.project ='wavenet_test'\n",
    "\n",
    "hparams.log_path = f'{BASE_DIR}/log.txt'\n",
    "\n",
    "\n",
    "hparams.SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau # scheduler\n",
    "hparams.scheduler_params = dict(      # scheduler params\n",
    "    mode='min',        \n",
    "    factor=0.2,\n",
    "    patience= 4 ,\n",
    "    verbose=True, \n",
    "    threshold=0.01,\n",
    "    threshold_mode='abs',\n",
    "    cooldown=0, \n",
    "    min_lr=1e-12,\n",
    "    eps=1e-08\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "m = MyModel(hparams).cuda()\n",
    "\n",
    "fitter = Fitter(m, hparams)\n",
    "fitter.fit(train_loader, valid_loader, False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
